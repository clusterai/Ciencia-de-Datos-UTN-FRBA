{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "__Universidad Tecnológica Nacional, Buenos Aires__<br/>\n",
    "__Ingeniería Industrial__<br/>\n",
    "__Cátedra de Ciencia de Datos - Curso I5521 - Turno Jueves Noche__<br/>\n",
    "__clase_09: Practica Clustering & PCA: Wine Dataset__<br/>\n",
    "__Elaborado por: Nicolas Aguirre__<br/>\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importamos Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias para trabajar.\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Importamos librerias de Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Importamos librerias de PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import rand_score, silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importamos Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Repositorio del Dataset**\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Wine\n",
    "\n",
    "**Wine Data Set:**\n",
    "\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombres de columnas\n",
    "names_col = [\n",
    "    \"G\",\n",
    "    \"Alcohol\",\n",
    "    \"Malic acid\",\n",
    "    \"Ash\",\n",
    "    \"Alcalinity of ash\",\n",
    "    \"Magnesium\",\n",
    "    \"Total phenols\",\n",
    "    \"Flavanoids\",\n",
    "    \"Nonflavanoid phenols\",\n",
    "    \"Proanthocyanins\",\n",
    "    \"Color intensity\",\n",
    "    \"Hue\",\n",
    "    \"OD280/OD315 of diluted wines\",\n",
    "    \"Proline\",\n",
    "]\n",
    "\n",
    "wine_df = pd.read_csv(\"wine.data\", delimiter=\",\", names=names_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wine_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las features que corresponden a X\n",
    "x = wine_df.iloc[:, 1:]\n",
    "y = wine_df.iloc[:, 0]\n",
    "display(x.head())\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a crear un dataframe para guardar los resultados\n",
    "results_df = pd.DataFrame(columns=[\"Cluster\", \"Rand_\", \"Sil_\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizamos un autoscaling con los datos, para todas las features\n",
    "scaler = StandardScaler().fit(x)\n",
    "xscal = scaler.transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dist_cent = []\n",
    "sil_list = []\n",
    "rand_list = []\n",
    "for k in range(2, 10):\n",
    "    # Creamos el objecto de cluster y lo fiteamos en la misma linea utilizado xscal\n",
    "    kmeans = KMeans(n_clusters=k, random_state=1).fit(xscal)\n",
    "    centers_i = kmeans.cluster_centers_  # Centroide de cada cluster\n",
    "    labels_i = kmeans.labels_  # Labels de cada muestra\n",
    "    # Silhouttte Score\n",
    "    sil_score_i = silhouette_score(xscal, labels_i)\n",
    "    sil_list.append(sil_score_i)\n",
    "    # Rand_Index\n",
    "    rand_index_i = rand_score(y, labels_i)\n",
    "    rand_list.append(rand_index_i)\n",
    "    dist_cent.append(kmeans.inertia_)\n",
    "# Plot de metricas\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axs[0].plot(range(2, 10), dist_cent, marker=\"s\")\n",
    "axs[0].set_xlabel(\"N° K\")\n",
    "axs[0].set_ylabel(\"Sum of squared distances\")\n",
    "# Silhoute plot\n",
    "axs[1].plot(range(2, 10), sil_list, marker=\"s\")\n",
    "axs[1].set_xlabel(\"N° K\")\n",
    "axs[1].set_ylabel(\"Silhouette\")\n",
    "# Rand Index plot\n",
    "axs[2].plot(range(2, 10), rand_list, marker=\"s\")\n",
    "axs[2].set_xlabel(\"N° K\")\n",
    "axs[2].set_ylabel(\"Rand Index\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P.C.A.\n",
    "\n",
    "\n",
    "**Generamos un PCA con los datos luego del autoscaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la cantidad de componentes\n",
    "n_comps = 13\n",
    "components = range(1, n_comps + 1)\n",
    "# Creamos el objeto PCA\n",
    "pca = PCA(n_components=n_comps)\n",
    "\n",
    "# Ajustamos\n",
    "pca.fit(xscal)\n",
    "# Transformamos\n",
    "xpca = pca.transform(xscal)\n",
    "\n",
    "# Porcentaje de la varianza explicada por cada Principal Component (PC)\n",
    "eigenvalues = pca.explained_variance_ratio_\n",
    "\n",
    "# Suma acumulada\n",
    "eigenvalues_acum = pca.explained_variance_ratio_.cumsum()\n",
    "\n",
    "# Graficamos\n",
    "# Eje Izquierdo\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "ax1.set_xlabel(\"Componentes Principales\", fontsize=20)\n",
    "ax1.set_ylabel(\"Varianza Explicada\", color=\"k\", fontsize=20)\n",
    "ax1.bar(components, eigenvalues, color=\"blue\")\n",
    "ax1.tick_params(axis=\"y\", labelcolor=\"blue\")\n",
    "\n",
    "# Eje derecho\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "ax2.set_ylabel(\"Varianza Acumulada\", color=\"k\", fontsize=20)\n",
    "ax2.plot(components, eigenvalues_acum, color=\"red\")\n",
    "ax2.tick_params(axis=\"y\", labelcolor=\"red\")\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvectors ($\\mathbf v$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De la libreria:\n",
    "# 'Principal axes in feature space, representing the directions of maximum variance in the data'\n",
    "# The components are sorted by explained_variance_\n",
    "pd.DataFrame(pca.components_[0:n_comps, :], columns=x.columns)\n",
    "\n",
    "# En criollo:\n",
    "# Es la direccion de los ejes de cada componente (autovectores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Scatter plot de los datos, solamente con 2 PC\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.scatter(xpca[:, 0], xpca[:, 1])\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.title(\"Figura de PC1 y PC2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta: Que cantidad de clusters seleccionariamos? Porque?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos un modelo de K means con ##RESPONDER## clusters con los datos autoscalados\n",
    "rta = 3\n",
    "kmeans = KMeans(n_clusters=rta, random_state=10).fit(xscal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualizamos los centroides finales de cada cluster\n",
    "centers = kmeans.cluster_centers_\n",
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Scatter plot de muestras y centroides con 2 PC: segun Cluster verdadero vs Clustering con K-Means\n",
    "\n",
    "# Verdadero\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.scatter(xpca[:, 0], xpca[:, 1], c=wine_df[\"G\"].astype(float))\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.title(\"Clustering Verdadero\")\n",
    "# K-Means\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.scatter(xpca[:, 0], xpca[:, 1], c=kmeans.labels_.astype(float))\n",
    "plt.scatter(centers[:, 0], centers[:, 1], marker=\"x\", color=\"r\", s=150)\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.title(\"Clustering K-Means\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREGUNTA:**\n",
    "\n",
    "**¿Son correctos los centroides?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Silhouttte Score\n",
    "sil_score = silhouette_score(xscal, kmeans.labels_)\n",
    "# Rand_Index\n",
    "rand_index = rand_score(y, kmeans.labels_)\n",
    "\n",
    "# Crear el nuevo DataFrame con los resultados a agregar\n",
    "new_row = pd.DataFrame(\n",
    "    {\"Cluster\": [\"Kmeans\"], \"Rand_\": [rand_index], \"Sil_\": [sil_score]}\n",
    ")\n",
    "\n",
    "# Concatenar el nuevo DataFrame al existente\n",
    "results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA + K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ahora vamos dejar de usar las variables originales.**\n",
    "\n",
    "**Vamos a clusterizar con lo que nos puedan explicar UNICAMENTE las primeras 2 PC, y compararemos los resultados.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cantidad de PC quer queremos\n",
    "reduced_dim = 2\n",
    "# Definimos nuetro nuevo X de dimension reducida\n",
    "xpca_rd = xpca[:, 0:reduced_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos el modelo PCA + K-means\n",
    "kmeans_rd = KMeans(n_clusters=3, random_state=10).fit(xpca_rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualizamos los centroides finales de cada cluster\n",
    "centers_rd = kmeans_rd.cluster_centers_\n",
    "centers_rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scatter plot con 2 PC:**\n",
    "\n",
    "**Cluster verdadero vs Clustering con PCA+K-Means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Verdadero\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.scatter(xpca[:, 0], xpca[:, 1], c=wine_df[\"G\"].astype(float))\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.title(\"Clustering Verdadero\")\n",
    "# PCA + K-Means\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.scatter(xpca_rd[:, 0], xpca_rd[:, 1], c=kmeans_rd.labels_.astype(float))\n",
    "plt.scatter(centers_rd[:, 0], centers_rd[:, 1], marker=\"x\", color=\"r\", s=150)\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\")\n",
    "plt.title(\"Clustering K-means+RD\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREGUNTA:**\n",
    "\n",
    "**¿Y aca que sucedió?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouttte Score\n",
    "sil_score = silhouette_score(xpca_rd, kmeans_rd.labels_)\n",
    "# Rand_Index\n",
    "rand_index = rand_score(y, kmeans_rd.labels_)\n",
    "\n",
    "\n",
    "# Crear un nuevo DataFrame con los resultados a agregar\n",
    "new_row = pd.DataFrame(\n",
    "    {\"Cluster\": [\"2 PC + Kmeans\"], \"Rand_\": [rand_index], \"Sil_\": [sil_score]}\n",
    ")\n",
    "\n",
    "# Concatenar el nuevo DataFrame con el existente\n",
    "results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "\n",
    "# Mostrar el DataFrame actualizado\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(pca.components_[0:reduced_dim, :], columns=x.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cantidad de PC quer queremos\n",
    "reduced_dim = 3\n",
    "xpca_rd = xpca[:, 0:reduced_dim]\n",
    "# Generamos el modelo PCA + K-means\n",
    "kmeans_rd = KMeans(n_clusters=3, random_state=10).fit(xpca_rd)\n",
    "centers_rd = kmeans_rd.cluster_centers_\n",
    "centers_rd\n",
    "\n",
    "# VS code: %matplotlib widget + pip install ipympl ipywidgets\n",
    "# Jupyter Notebook: %matplotlib notebook\n",
    "%matplotlib notebook\n",
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax = fig.add_subplot(projection=\"3d\")\n",
    "ax.scatter3D(xpca[:, 0], xpca[:, 1], xpca[:, 2], c=kmeans_rd.labels_)\n",
    "ax.scatter3D(\n",
    "    centers_rd[:, 0], centers_rd[:, 1], centers_rd[:, 2], marker=\"o\", color=\"r\", s=150\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"PC1\")\n",
    "ax.set_ylabel(\"PC2\")\n",
    "ax.set_zlabel(\"PC3\")\n",
    "ax.set_xticklabels([])\n",
    "ax.set_yticklabels([])\n",
    "ax.set_zticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouttte Score\n",
    "sil_score = silhouette_score(xpca_rd, kmeans_rd.labels_)\n",
    "# Rand_Index\n",
    "rand_index = rand_score(y, kmeans_rd.labels_)\n",
    "\n",
    "new_row = pd.DataFrame(\n",
    "    {\"Cluster\": [\"3 PC + Kmeans\"], \"Rand_\": [rand_index], \"Sil_\": [sil_score]}\n",
    ")\n",
    "\n",
    "# Guardamos los resultados\n",
    "results_df = pd.concat([results_df, new_row])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta:**\n",
    "\n",
    "**Que Conclusiones sacamos ??**\n",
    "\n",
    "**Cual es la cantidad maxima de PC que podriamos usar? Porque?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recontruccion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Si quisieramos volver a nuestro espacio original, consideremos que:**\n",
    "\n",
    "$Z = X \\cdot \\mathbf{V} \\rightarrow \\hat X = Z \\cdot \\mathbf{V}^T$\n",
    "\n",
    "siendo $Z$ la proyeccion de X en el nuevo subspace.\n",
    "\n",
    "Como vimos en la clase, los datos tienen que ser centrados (como minimo!)\n",
    "\n",
    "Para cumplir con las ecuaciones y volver a tener nuestra $\\hat X$, deberiamos sumarle las medias ...\n",
    "\n",
    "**A)** $\\hat X = Z \\cdot \\mathbf{V}^T + \\mu$\n",
    "\n",
    "Pero en nuestro caso Standarizamos los datos ($\\mu = 0, \\sigma = 1$). Entonces debemos re escalar teniendo en cuenta $\\sigma$ antes de sumar $\\mu$.\n",
    "\n",
    "**B)** $\\hat X = ( Z  \\cdot \\mathbf{V}^T) \\cdot \\sigma + \\mu $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_array = x.values  # Convertir x a un numpy array\n",
    "\n",
    "# Calcular la media y la desviación estándar\n",
    "mu = np.mean(x_array, axis=0)\n",
    "std = np.std(x_array, axis=0)\n",
    "\n",
    "# 2 PCA\n",
    "reduced_dim = 13\n",
    "xpca_rd = xpca[:, 0:reduced_dim]\n",
    "x_rec = np.dot(xpca_rd, pca.components_[0:reduced_dim, :])\n",
    "\n",
    "# Normalizar la reconstrucción con la desviación estándar y agregar la media\n",
    "x_rec = x_rec * std\n",
    "x_rec += mu\n",
    "\n",
    "# Convertir el resultado a DataFrame con las columnas originales\n",
    "x_rec_df = pd.DataFrame(x_rec, columns=x.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples de la reconstruccion\n",
    "sample = np.random.randint(0, high=x.shape[0])\n",
    "\n",
    "display(x.iloc[sample, :].to_frame().transpose())\n",
    "display(x_rec_df.iloc[sample, :].to_frame().transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resumen**:\n",
    "\n",
    "* Cuando los datos son centrados, la reconstruccion se hace teniendo en cuenta que:\n",
    "\n",
    "**A)**  $\\hat X = Z \\cdot \\mathbf{V}^T + \\mu $\n",
    "\n",
    "\n",
    "* Cuando ademas los datos son standarizados, la reconstruccion se hace teniendo en cuenta que:\n",
    "\n",
    "**B)**   $\\hat X = (Z \\cdot \\mathbf{V}^T) \\cdot \\sigma  + \\mu $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preguntas:**\n",
    "\n",
    "**Cuando debemos centrar y cuando estandarizar?**\n",
    "\n",
    "**La presencia de outliers puede modificar el resultado del PCA? Porque?**\n",
    "\n",
    "**Podemos hacer PCA sobre features categoricas?**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
