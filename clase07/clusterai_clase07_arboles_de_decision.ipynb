{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_GuIjb3z3aZ"
      },
      "source": [
        "____\n",
        "__Universidad Tecnologica Nacional, Buenos Aires__<br/>\n",
        "__Ingeniería Industrial__<br/>\n",
        "__Cátedra de Ciencia de Datos - Curso I5521 - Turno Jueves a la noche__<br/>\n",
        "__Clase_07: Árboles de decisión__<br/>\n",
        "__Autor: Santiago Chas__\n",
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXHw2hEQz3ae"
      },
      "source": [
        "## Prediccion del precio de las propiedades"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAs3xusGz3ae"
      },
      "source": [
        "Vamos a utilizar la informacion de https://www.properati.com.ar/ correspondiente a Departamentos y PH de Capital Federal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hZC592Tz3af"
      },
      "source": [
        "#### Importamos librerias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#pip install openpyxl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G66f_NYDz3af"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Configura la opción para desactivar la notación científica\n",
        "pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "\n",
        "\n",
        "# Configurar opciones para poder visualizar todas las columnas\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYiXyliUz3ah"
      },
      "outputs": [],
      "source": [
        "# Importamos el dataset e imprimimos algunos registros\n",
        "df = pd.read_csv(\"./properati_capital.csv\")\n",
        "df.sample(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9actxtfz3ah"
      },
      "outputs": [],
      "source": [
        "# Vemos las dimensiones del dataset\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHvlmO9oz3ai"
      },
      "source": [
        "### EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PFlVTYMz3ai"
      },
      "source": [
        "#### Distribucion del precio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_ja8LoSz3ai"
      },
      "outputs": [],
      "source": [
        "# Crear un boxplot para la columna \"price\"\n",
        "\n",
        "plt.figure(figsize=(8, 6))  # Tamaño opcional del gráfico\n",
        "plt.boxplot(df['price'])\n",
        "plt.title('Boxplot de Precio')\n",
        "plt.ylabel('Precio')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWxBk5Jdz3aj"
      },
      "outputs": [],
      "source": [
        "# Analizamos estadisticamente la columna precio\n",
        "df.price.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9pDgbpZz3aj"
      },
      "source": [
        "Como vemos, tenemos valores atípicos.\n",
        "\n",
        "Por este motivo vamos a evaluar los percentiles 99 y 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-3-DXo9z3aj"
      },
      "outputs": [],
      "source": [
        "# Calcular el percentil 99 de la columna 'price'\n",
        "\n",
        "percentil_99 = df['price'].quantile(0.99)\n",
        "print(f\"El valor del percentil 99 es: {percentil_99}\")\n",
        "percentil_01 = df['price'].quantile(0.01)\n",
        "print(f\"El valor del percentil 1 es: {percentil_01}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIq2HkxWz3ak"
      },
      "outputs": [],
      "source": [
        "# Nos vamos a quedar con los registros que esten entre estos valores\n",
        "\n",
        "df = df[(df['price'] > percentil_01) & (df['price'] < percentil_99)]\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYv4fDqbz3ak"
      },
      "outputs": [],
      "source": [
        "# Crear un boxplot para la columna \"price\"\n",
        "plt.figure(figsize=(8, 6))  # Tamaño opcional del gráfico\n",
        "plt.boxplot(df['price'])\n",
        "plt.title('Boxplot de Precio')\n",
        "plt.ylabel('Precio')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.price.describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYKEr_wRz3ak"
      },
      "source": [
        "## EDA y Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El Feature Engineering es el conjunto de técnicas destinadas a transformar y preparar los datos antes de que sean utilizados por un modelo de aprendizaje automático. Su objetivo principal es mejorar la calidad y relevancia de las variables (features) para que el modelo pueda aprender patrones de manera más efectiva.\n",
        "Este proceso incluye tareas que ya vimos, como:\n",
        "\n",
        "- Estandarización o normalización de los datos numéricos.\n",
        "- Codificación de variables categóricas (por ejemplo, One-Hot Encoding).\n",
        "\n",
        "Pero también abarca otras estrategias más avanzadas, como:\n",
        "\n",
        "- Creación de nuevas variables a partir de las existentes (feature creation).\n",
        "- Selección de características relevantes (feature selection).\n",
        "- Manejo de outliers y valores faltantes\n",
        "- Transformaciones no lineales\n",
        "- Incorporación de datos externas\n",
        "\n",
        "\n",
        "En esta notebook vamos a explorar varias de estas técnicas.\n",
        "\n",
        "Pueden leer más aca: \n",
        "- https://www.analyticsvidhya.com/blog/2021/10/a-beginners-guide-to-feature-engineering-everything-you-need-to-know/\n",
        "- https://builtin.com/articles/feature-engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwGMRdkKz3ak"
      },
      "outputs": [],
      "source": [
        "# Imprimimos los nombres de variables\n",
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ct7nPFkiz3al"
      },
      "outputs": [],
      "source": [
        "# Eliminamos las variables \"Unnamed: 0\" y \"id\" ya que son variables keys del dataset y no nos aportan informacion\n",
        "\n",
        "ids_vars = [\"Unnamed: 0\", \"id\"] # creo una lista de las variables que decidimos eliminar\n",
        "df.drop(ids_vars, axis=1,inplace= True) # el parametro inplace nos permite guardar los nuevos cambios dentro de la variable existente\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1Q8gnkiz3al"
      },
      "source": [
        "Veamos ahora los tipos de datos de estas columnas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRxVIuakz3al"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHpfk5c5z3al"
      },
      "source": [
        "Vemos que existen variables con muchos valores nulos.\n",
        "\n",
        "Hagamos este análisis de una manera mas precisa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FC4Ug6BYz3al"
      },
      "outputs": [],
      "source": [
        "# Calculamos % los valores faltantes para cada una de las variables\n",
        "miss = pd.DataFrame(df.isnull().mean(), columns=[\"Missing\"])\n",
        "perc_miss = miss.loc[miss.Missing > 0]\n",
        "perc_miss.sort_values(\"Missing\", ascending = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol2rulbiz3al"
      },
      "source": [
        "Observamos que las variables l6, l5, price_period y l4 contienen un alto % de valores faltantes.\n",
        "\n",
        "Por este motivo vamos a eliminarlos de nuestro dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHLsmPrxz3am"
      },
      "outputs": [],
      "source": [
        "# Repetimos el método de eliminar columnas\n",
        "\n",
        "vars_nulls = [\"l6\", \"l5\", \"l4\", \"price_period\"]\n",
        "df.drop(vars_nulls, axis = 1, inplace= True )\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNARTkdTz3am"
      },
      "outputs": [],
      "source": [
        "df.tail(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoKuaaoyz3am"
      },
      "source": [
        "Sospechamos que existen columnas con un unico valor de dato.\n",
        "\n",
        "Si es asi estas columnas no nos aportan informacion para nuestra futura prediccion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsSXr9yoz3am"
      },
      "outputs": [],
      "source": [
        "# Contamos la cantidad de valores unicos que tiene cada columna\n",
        "\n",
        "for x in df.columns:\n",
        "    unique_vals = df[f\"{x}\"].nunique() # nunique() cuenta la cantidad de valores unicos que tiene una columna\n",
        "    if unique_vals > 2:\n",
        "        None\n",
        "    else:\n",
        "        print(f'Variable {x}:', unique_vals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N--LbUA1z3am"
      },
      "source": [
        "Podemos ver esto mismo usando el método value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Uwl0qVDz3am"
      },
      "outputs": [],
      "source": [
        "# Vemos los valores para property_type y para l1\n",
        "\n",
        "df.l1.value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQxkgbbDz3am"
      },
      "outputs": [],
      "source": [
        "df.property_type.value_counts(dropna=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6tyJgaGz3am"
      },
      "source": [
        "Property_type tiene sentido que solo tenga dos valores posibles ya que son los tipos de propiedades que queremos predecir.\n",
        "\n",
        "Pero el resto de las variables que tienen valores unicos las eliminaremos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xAuTwNLz3an"
      },
      "outputs": [],
      "source": [
        "unique_vars = [\"ad_type\", \"l1\", \"l2\", \"currency\", \"operation_type\"]\n",
        "df.drop(unique_vars, axis = 1, inplace= True)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAsohvCrz3an"
      },
      "source": [
        "Las variables \"created_on\", \"start_date\", \"end_date\" no son variables útiles para lo que queremos hacer --> las eliminamos\n",
        "\n",
        "Se podrían crear variables muy relevantes para nuestro modelo a partir de la latitud y la longitud pero no las utilizaremos en nuestro entrenamiento.\n",
        "\n",
        "Algunas de las cosas que se pueden hacer pueden hacer pueden encontrarlas en este post: https://python.plainenglish.io/how-to-use-geo-location-for-feature-engineering-using-near-by-points-of-interest-563752e6ad26"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2TkZsT2z3an"
      },
      "outputs": [],
      "source": [
        "drop_vars = [\"created_on\", \"start_date\", \"end_date\", \"lat\", \"lon\"]\n",
        "df.drop(drop_vars, axis=1, inplace= True)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1ANDZLtz3an"
      },
      "source": [
        "#### Enriquecimiento del dataset con una fuente externa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoBq81xVz3an"
      },
      "source": [
        "Importamos un dataset que extraemos de https://mudafy.com.ar/\n",
        "\n",
        "Este contiene la informacion del precio promedio de m2 por barrio y la Comuna que corresponde para cada barrio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7p9Lqg9Cz3ao"
      },
      "outputs": [],
      "source": [
        "df_m2 = pd.read_excel(\"./precio_prom_m2.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjLggWOIz3ao"
      },
      "outputs": [],
      "source": [
        "df_m2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npEN8JKPz3at"
      },
      "outputs": [],
      "source": [
        "df_m2.info(verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8BfqHT5z3at"
      },
      "outputs": [],
      "source": [
        "# Joineamos los dos df para traer las nuevas variables que seran de utildiad\n",
        "\n",
        "df_merged = pd.merge(df, df_m2, left_on = \"l3\", right_on = \"Barrio\", how= 'left')\n",
        "df_merged.drop([\"Barrio\"], axis=1, inplace=True)  # Eliminamos la variable Barrio ya que nos proporciona el mismo tipo de info que l3\n",
        "df_merged.rename(columns={\"Valor m2 (USD)\":\"valor_m2\"}, inplace=True) # Renombramos la variable por un nombre mas sencillo\n",
        "df_merged.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_merged.drop([\"Comuna\"], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0U3ZBygz3at"
      },
      "source": [
        "#### Tratamiento de valores faltantes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XWhZnLbz3au"
      },
      "source": [
        "Volvemos hacer el análisis de missings para ver que tan poblada estan las nuevas variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEHJ5RmNz3au"
      },
      "outputs": [],
      "source": [
        "miss = pd.DataFrame(df_merged.isnull().mean(), columns=[\"Missing\"])\n",
        "perc_miss = miss.loc[miss.Missing > 0]\n",
        "perc_miss.sort_values(\"Missing\", ascending = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owzhwiLqz3au"
      },
      "outputs": [],
      "source": [
        "# Analizamos algunos valores estadísticos de las variables \"bathrooms\", \"rooms\", \"bedrooms\", \"surface_total\", \"surface_covered\", \"valor_m2\"\n",
        "\n",
        "df_merged[[\"bathrooms\", \"rooms\", \"bedrooms\", \"surface_total\", \"surface_covered\", \"valor_m2\"]].describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9tMuQ6Sz3au"
      },
      "source": [
        "Vemos que existen outliers para algunas de estas columnas, pero esto lo analizaremos luego."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewh2ZE1az3au"
      },
      "source": [
        "Ahora nos concentraremos en la imputacion de valores faltantes aplicando distintas estrategias de imputacion segun la variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KS4c7ASz3au"
      },
      "outputs": [],
      "source": [
        "# Para bathrooms, room y bedrooms --> imputaremos con la mediana ya que deberian ser valores enteros\n",
        "\n",
        "\n",
        "# Calculamos la media de cada columna\n",
        "median_bathrooms = df_merged['bathrooms'].median()\n",
        "median_rooms = df_merged['rooms'].median()\n",
        "median_bedrooms = df_merged['bedrooms'].median()\n",
        "\n",
        "# Imprime las medias\n",
        "print(f\"Mediana de 'bathrooms': {median_bathrooms}\")\n",
        "print(f\"Mediana de 'rooms': {median_rooms}\")\n",
        "print(f\"Mediana de 'bedrooms': {median_bedrooms}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbXMemBHz3au"
      },
      "outputs": [],
      "source": [
        "# Imputamos los valores\n",
        "\n",
        "df_merged['bathrooms'].fillna(median_bathrooms, inplace=True)\n",
        "df_merged['rooms'].fillna(median_rooms, inplace=True)\n",
        "df_merged['bedrooms'].fillna(median_bedrooms, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LrZOlbBTz3au"
      },
      "outputs": [],
      "source": [
        "# Como surface_total, surface_covered y valor_m2 pueden tomar valores decimales utilizamos la media para imputar sus nulos\n",
        "\n",
        "# Calculamos las medias para cada columna\n",
        "mean_surface_total = df_merged['surface_total'].mean()\n",
        "mean_surface_covered = df_merged['surface_covered'].mean()\n",
        "mean_valor_m2 = df_merged['valor_m2'].mean()\n",
        "\n",
        "# Imprimimos las medias\n",
        "print(f\"Media de 'surface_total': {mean_surface_total}\")\n",
        "print(f\"Media de 'surface_covered': {mean_surface_covered}\")\n",
        "print(f\"Media de 'valor_m2': {mean_valor_m2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhfvVJ-sz3av"
      },
      "outputs": [],
      "source": [
        "# Imputamos los valores\n",
        "\n",
        "df_merged['surface_total'].fillna(mean_surface_total, inplace=True)\n",
        "df_merged['surface_covered'].fillna(mean_surface_covered, inplace=True)\n",
        "df_merged['valor_m2'].fillna(mean_valor_m2, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBVvawi9z3av"
      },
      "source": [
        "#### Imputacion de missings para variables categoricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wcnpX9kz3av"
      },
      "outputs": [],
      "source": [
        "df_merged['l3'].fillna('sin_valor', inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBoZaGDEz3av"
      },
      "outputs": [],
      "source": [
        "# Verificamos los valores faltantes\n",
        "\n",
        "df_merged.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSbkVq0zz3av"
      },
      "source": [
        "#### Tratamiento de valores atípicos/outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5B_TOhYfz3aw"
      },
      "outputs": [],
      "source": [
        "# Volvemos a imprimir algunas metricas estadísticas para \"bathrooms\", \"rooms\", \"bedrooms\", \"surface_total\", \"surface_covered\"\n",
        "\n",
        "df_merged[[\"bathrooms\", \"rooms\", \"bedrooms\", \"surface_total\", \"surface_covered\"]].describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsH8Or2Sz3aw"
      },
      "source": [
        "Vamos a utilizar la tecnica de capping para no perder registros para nuestro entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDZ0T0Nuz3aw"
      },
      "outputs": [],
      "source": [
        "# Capping superior\n",
        "# Lista de columnas a procesar\n",
        "cap_sup_cols = [\"bathrooms\", \"rooms\", \"bedrooms\", \"surface_total\", \"surface_covered\"]\n",
        "\n",
        "# Iteramos a través de las columnas y reemplazamos los valores que superan el percentil 99\n",
        "for columna in cap_sup_cols:\n",
        "    percentil_99 = np.percentile(df_merged[columna], 99)\n",
        "    df_merged[columna] = np.where(df_merged[columna] > percentil_99, percentil_99, df_merged[columna])\n",
        "\n",
        "df_merged[[\"bathrooms\", \"rooms\", \"bedrooms\", \"surface_total\", \"surface_covered\"]].describe().T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3P1EJTLz3ax"
      },
      "outputs": [],
      "source": [
        "# Capping inferior\n",
        "\n",
        "cap_inf_cols = [\"surface_total\", \"surface_covered\"]\n",
        "\n",
        "# Itera a través de las columnas y reemplaza los valores según la condición\n",
        "for columna in cap_inf_cols:\n",
        "    percentil_1 = np.percentile(df_merged[columna], 1)\n",
        "    df_merged[columna] = np.where(df_merged[columna] < percentil_1, percentil_1, df_merged[columna])\n",
        "\n",
        "df_merged[[\"bathrooms\", \"rooms\", \"bedrooms\", \"surface_total\", \"surface_covered\"]].describe().T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhX86FOkz3ax"
      },
      "source": [
        "#### Búsqueda de keywords en variables de texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNWjKl6Jz3ax"
      },
      "outputs": [],
      "source": [
        "# Concatenamos las variables del titulo y la descripcion para trabajar con una unica variable\n",
        "\n",
        "df_merged['title_descr'] = df_merged['description'] + ' ' + df_merged['title']\n",
        "df_merged.drop([\"description\", \"title\"], axis=1, inplace=True) # eliminamos las variables originales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBk0OjoNz3ay"
      },
      "source": [
        "Utilizaremos la libreria regex que es de expresiones regulares.\n",
        "\n",
        "Documentacion: https://docs.python.org/3/library/re.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vbf7Qr1wz3ay"
      },
      "outputs": [],
      "source": [
        "# Importamos la libreria re (regex)\n",
        "import re\n",
        "\n",
        "# Definimos las palabras clave\n",
        "keywords = [\"sum\", \"pileta\", \"amenities\", \"cochera\"]\n",
        "\n",
        "# Crea una columna para cada palabra clave y asigna 0 como valor inicial\n",
        "for keyword in keywords:\n",
        "    df_merged[keyword] = 0\n",
        "\n",
        "# Itera a través de las palabras clave y asigna 1 si se encuentra la palabra clave en 'title_descr'\n",
        "for keyword in keywords:\n",
        "    # La siguiente línea crea un patrón de búsqueda de expresión regular (regex) para la palabra clave actual.\n",
        "    # - El patrón \\b{}\\\\b busca la palabra clave completa como una palabra independiente.\n",
        "    # - El flag re.IGNORECASE hace que la búsqueda sea insensible a mayúsculas y minúsculas.\n",
        "    pattern = re.compile(r'\\b{}\\b'.format(keyword), flags=re.IGNORECASE) #\n",
        "    df_merged[keyword] = df_merged['title_descr'].apply(lambda x: 1 if pattern.search(str(x)) else 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4x2Hkjmz3ay"
      },
      "outputs": [],
      "source": [
        "df_merged.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "INCmMYOxz3ay"
      },
      "outputs": [],
      "source": [
        "# Iteramos a través de las columnas creadas y realiza un value_counts para cada una\n",
        "\n",
        "for keyword_column in keywords:\n",
        "    value_counts_keyword = df_merged[keyword_column].value_counts()\n",
        "    print(f\"Value Counts para '{keyword_column}':\")\n",
        "    print(value_counts_keyword)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XcJC43pz3ay"
      },
      "outputs": [],
      "source": [
        "# Ahora que tenemos nuestras columnas creadas podemos eliminar la columna de texto\n",
        "df_merged.drop([\"title_descr\"], axis= 1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOpKFpE8z3az"
      },
      "outputs": [],
      "source": [
        "\n",
        "df_merged.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Eliminamos la columna de barrios porque tienen muchos valores posibles. \n",
        "# TAREA: Agrupar los barrios bajo algun criterio de no mas de 5 valores posibles y ejecutar los modelos para ver si mejoran las predicciones\n",
        "df_merged.drop([\"l3\"], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWwI-khqz3az"
      },
      "source": [
        "#### Variables Categóricas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3Vm-POPz3az"
      },
      "outputs": [],
      "source": [
        "y = df_merged[[\"price\"]]\n",
        "x = df_merged.drop(['price'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJtfRUUHz3a0"
      },
      "outputs": [],
      "source": [
        "x.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvUPTHLoz3a0"
      },
      "outputs": [],
      "source": [
        "x = pd.get_dummies(data=x, columns=['property_type'])\n",
        "x.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fmaD-G7z3a0"
      },
      "outputs": [],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csDzM63-z3a0"
      },
      "source": [
        "### Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7SW-ohbz3a0"
      },
      "outputs": [],
      "source": [
        "# Importamos librerias de Machine Learning\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR, LinearSVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error, make_scorer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV # Hace una busqueda de una cantidad determinada de parametros en lugar de todas las combinaciones posibles\n",
        "\n",
        "# Documentacion de RandonSearchCV: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBDDxjZFz3a1"
      },
      "outputs": [],
      "source": [
        "# Train Test Split\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.20, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2NWBL7_z3a1"
      },
      "source": [
        "#### Escalo la data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyKvdzkvz3a1"
      },
      "outputs": [],
      "source": [
        "scaler = preprocessing.StandardScaler().fit(xtrain)\n",
        "xtrain_scal = scaler.transform(xtrain)\n",
        "xtest_scal = scaler.transform(xtest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3RMsyyYz3a1"
      },
      "source": [
        "#### Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IagZnnuvz3a1"
      },
      "outputs": [],
      "source": [
        "lr = LinearRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_hS49JNz3a2"
      },
      "outputs": [],
      "source": [
        "lr.fit(xtrain_scal, ytrain)\n",
        "ypred = lr.predict(xtest_scal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6iCOZajz3a2"
      },
      "outputs": [],
      "source": [
        "np.sqrt(mean_squared_error(ytest, ypred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5Qd73VJz3a2"
      },
      "source": [
        "#### KNN Regressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmdCztMbz3a2"
      },
      "outputs": [],
      "source": [
        "knn = KNeighborsRegressor(weights = \"distance\")\n",
        "parameters_k = np.arange(20,41,5)\n",
        "parameters_knn = [{'n_neighbors': parameters_k}]\n",
        "regressor_knn = GridSearchCV(knn, parameters_knn, refit = True, cv=5, scoring=\"neg_mean_squared_error\", verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3SgOsjnz3a2"
      },
      "outputs": [],
      "source": [
        "regressor_knn.fit(xtrain_scal, ytrain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGu--R7Hz3a2"
      },
      "outputs": [],
      "source": [
        "regressor_knn.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClLI_ebBz3a3"
      },
      "outputs": [],
      "source": [
        "regressor_knn.best_score_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2AaS4O1z3a3"
      },
      "outputs": [],
      "source": [
        "ypred2 = regressor_knn.predict(xtest_scal)\n",
        "np.sqrt(mean_squared_error(ytest, ypred2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Ensamble de árboles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Recuperar nombres de columnas desde xtrain\n",
        "column_names = xtrain.columns\n",
        "\n",
        "\n",
        "# Convertir arrays escalados en DataFrames con nombres de columnas\n",
        "xtrain_scal_df = pd.DataFrame(xtrain_scal, columns=column_names)\n",
        "xtest_scal_df = pd.DataFrame(xtest_scal, columns=column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir el modelo base\n",
        "rf = RandomForestRegressor(random_state=42)\n",
        "\n",
        "# Definir el espacio de búsqueda de hiperparámetros\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 500],\n",
        "    'max_depth': [10, 20, 30, 50],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    \n",
        "}\n",
        "\n",
        "# Configurar RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=1\n",
        ")\n",
        "\n",
        "# Entrenar el modelo\n",
        "random_search.fit(xtrain_scal_df, ytrain)\n",
        "\n",
        "# Evaluar en el conjunto de test\n",
        "best_model_rf = random_search.best_estimator_\n",
        "y_pred = best_model_rf.predict(xtest_scal)\n",
        "rmse = np.sqrt(mean_squared_error(ytest, y_pred))\n",
        "\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Test RMSE:\", rmse)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los modelos de árboles nos permiten ver la importancia de las variables. Vamos a analizarlo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener nombres de las variables desde el DataFrame con columnas\n",
        "feature_names = xtrain_scal_df.columns\n",
        "\n",
        "# Obtener importancia de variables\n",
        "rf_importance = best_model_rf.feature_importances_\n",
        "\n",
        "# Crear DataFrame ordenado\n",
        "df_rf = pd.DataFrame({'Feature': feature_names, 'Importance': rf_importance}).sort_values(by='Importance', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_rf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import xgboost as xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir el modelo base\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "\n",
        "# Espacio de búsqueda de hiperparámetros\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 500],\n",
        "    'max_depth': [3, 4, 5, 6],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "}\n",
        "\n",
        "# Configurar RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=xgb_model,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=1  # evitar problemas en algunos entornos\n",
        ")\n",
        "\n",
        "# Entrenar el modelo\n",
        "random_search.fit(xtrain_scal, ytrain)\n",
        "\n",
        "# Evaluar en el conjunto de test\n",
        "best_model_xgb = random_search.best_estimator_\n",
        "y_pred = best_model_xgb.predict(xtest_scal)\n",
        "rmse = np.sqrt(mean_squared_error(ytest, y_pred))\n",
        "\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Test RMSE:\", rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener nombres de las variables desde el DataFrame con columnas\n",
        "feature_names = xtrain_scal_df.columns\n",
        "\n",
        "# Obtener importancia de variables\n",
        "xgb_importance = best_model_xgb.feature_importances_\n",
        "\n",
        "# Crear DataFrame ordenado\n",
        "df_xgb = pd.DataFrame({'Feature': feature_names, 'Importance': xgb_importance}).sort_values(by='Importance', ascending=False)\n",
        "df_xgb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### LightGBM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#pip install lightgbm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import lightgbm as lgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir el modelo base\n",
        "lgbm = lgb.LGBMRegressor(random_state=42)\n",
        "\n",
        "# Espacio de búsqueda de hiperparámetros\n",
        "param_dist = {\n",
        "    'num_leaves': [31, 50, 70],\n",
        "    'max_depth': [3, 4, 5, 6],\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'subsample': [0.6, 0.8, 1.0],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "# Configurar RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=lgbm,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=1  # evitar problemas en algunos entornos\n",
        ")\n",
        "\n",
        "# Entrenar el modelo\n",
        "random_search.fit(xtrain_scal, ytrain)\n",
        "\n",
        "# Evaluar en el conjunto de test\n",
        "best_model_lgbm = random_search.best_estimator_\n",
        "y_pred = best_model_lgbm.predict(xtest_scal)\n",
        "rmse = np.sqrt(mean_squared_error(ytest, y_pred))\n",
        "\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "print(\"Test RMSE:\", rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Obtener nombres de las variables desde el DataFrame con columnas\n",
        "feature_names = xtrain_scal_df.columns\n",
        "\n",
        "# Obtener importancia de variables\n",
        "lgbm_importance = best_model_lgbm.feature_importances_\n",
        "\n",
        "# Crear DataFrame ordenado\n",
        "df_lgbm = pd.DataFrame({'Feature': feature_names, 'Importance': lgbm_importance}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Normalizar importancia en porcentaje\n",
        "df_lgbm['Importance (%)'] = 100 * df_lgbm['Importance'] / np.sum(df_lgbm['Importance'])\n",
        "df_lgbm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Tarea: Evaluar y probar que manera se pueden mejorar las performances de los modelos agregando nuevas variables y evaluando otros hiperparámetros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
