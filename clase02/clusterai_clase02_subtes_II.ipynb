{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "**Universidad Tecnológica Nacional, Buenos Aires**<br/>\n",
    "**Ingeniería Industrial**<br/>\n",
    "**Cátedra de Ciencia de Datos - Curso I5521 - Turno jueves noche**<br/>\n",
    "**Clase_02:** EDA : Analisis Exploratorio de los Datos<br/>\n",
    "**Elaborado por:** Nicolas Aguirre\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import utils as ut\n",
    "from scipy.stats import skew\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset: Subtes de CABA - Molinetes 2017-2023\n",
    "\n",
    "Fuente: https://data.buenosaires.gob.ar/dataset/subte-viajes-molinetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es una buena practica definir al inicio de nuestro script/notebook las rutas de los archivos que vamos a utilizar, como asi tambien las rutas donde vamos a guardar los resultados de nuestro analisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicion de la ruta base de los datos\n",
    "base_path = \"path/to/your/data/\"\n",
    "plot_path = \"path/where/you/want/to/save/plots/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diccionario Auxiliar con las rutas de los archivos de molinetes\n",
    "\n",
    "\n",
    "Dado que la forma en la que los archivos estan disponiubles cambia a partir de 2022, vamos a tener dos diccionarios\n",
    "uno para los archivos `viejos` y otro para los `nuevos`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archivos viejos: un solo archivo por año\n",
    "molientes_paths_olds = {\n",
    "    \"2017\": base_path + \"molinetes-2017/molinetes.csv\",\n",
    "    \"2018\": base_path + \"molinetes-2018/molinetes.csv\",\n",
    "    \"2019\": base_path + \"molinetes-2019/molinetes.csv\",\n",
    "    #        \"2020\": base_path + \"molinetes-2020/molinetes.csv\",\n",
    "    \"2021\": base_path + \"molinetes-2021/molinetes.csv\",\n",
    "}\n",
    "\n",
    "# Archivos nuevos: un directorio con varios archivos dentro por año\n",
    "molientes_paths_news = {\n",
    "    \"2022\": base_path + \"molinetes-2022\",\n",
    "    \"2023\": base_path + \"molinetes-2023\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploremos rapidamente las columnas y como estna conformados los datos para cada grupo de años. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2017-2021\n",
    "df = pd.read_csv(molientes_paths_olds[\"2017\"], delimiter=\",\", encoding=\"latin-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2022-2023\n",
    "df = pd.read_csv(\n",
    "    molientes_paths_news[\"2023\"] + \"/fixed_202306_PAX15min-ABC.csv\",\n",
    "    delimiter=\";\",\n",
    "    engine=\"python\",\n",
    "    encoding=\"latin-1\",\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede obervar que a partir del año 2022, los nombres y tipos de datos de las columnas cambiaron.\n",
    "El cientifico de datos debe estar atento a estos cambios, ya que pueden afectar el análisis y los resultados obtenidos.\n",
    "En tal sentido la actividad de pre-procesamiento de los datos es fundamental en pos de consolidar un dataset coherente y consistente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estaciones de Subte por Línea (ordenado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los nombres unificados de las estaciones por cada linea\n",
    "orden_linea_a = [\"Plaza de Mayo\", \"Peru\", \"Piedras\", \"Lima\", \"Saenz Peña\", \"Congreso\", \"Pasco\", \"Alberti\", \"Plaza Miserere\", \"Loria\", \"Castro Barros\", \"Rio de Janeiro\", \"Acoyte\", \"Primera Junta\", \"Puan\", \"Carabobo\", \"Flores\", \"San Pedrito\",]\n",
    "orden_linea_b = [\"Leandro N. Alem\", \"Florida\", \"Carlos Pellegrini\", \"Uruguay\", \"Callao\", \"Pasteur\", \"Pueyrredon\", \"Carlos Gardel\", \"Medrano\", \"Angel Gallardo\", \"Malabia\", \"Dorrego\", \"Federico Lacroze\", \"Tronador\", \"Los Incas\", \"Echeverria\", \"Rosas\",]\n",
    "orden_linea_c = [\"Constitucion\", \"San Juan\", \"Independencia\", \"Mariano Moreno\", \"Avenida de Mayo\", \"Diagonal Norte\", \"Lavalle\", \"General San Martin\", \"Retiro\",]\n",
    "orden_linea_d = [\"Catedral\", \"9 de Julio\", \"Tribunales\", \"Callao\", \"Facultad de Medicina\", \"Pueyrredon\", \"Aguero\", \"Bulnes\", \"Scalabrini Ortiz\", \"Plaza Italia\", \"Palermo\", \"Ministro Carranza\", \"Olleros\", \"Jose Hernandez\", \"Juramento\", \"Congreso de Tucuman\",]\n",
    "orden_linea_e = [\"RETIRO\", \"CORREO CENTRAL\", \"CATALINAS\", \"Bolivar\", \"GENERAL BELGRANO\", \"Independencia\", \"San Jose\", \"Entre Rios\", \"Pichincha\", \"Jujuy\", \"URQUIZA\", \"Boedo\", \"Avenida La Plata\", \"JOSE MARIA MORENO\", \"EMILIO MITRE\", \"MEDALLA MILAGROSA\", \"VARELA\", \"PZA. DE LOS VIRREYES\",]\n",
    "orden_linea_h = list(reversed([ \"Hospitales\", \"PATRICIOS\", \"Caseros\", \"Inclan\", \"Humberto I\", \"Venezuela\", \"Once\", \"Corrientes\", \"Cordoba\", \"Santa Fe\", \"Las Heras\", \"Facultad de Derecho\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario auxiliar que contenga los nombres ordenados de las estaciones de cada linea\n",
    "orden_linea = {\n",
    "    \"A\": [x.upper() for x in orden_linea_a],\n",
    "    \"B\": [x.upper() for x in orden_linea_b],\n",
    "    \"C\": [x.upper() for x in orden_linea_c],\n",
    "    \"D\": [x.upper() for x in orden_linea_d],\n",
    "    \"E\": [x.upper() for x in orden_linea_e],\n",
    "    \"H\": [x.upper() for x in orden_linea_h],\n",
    "}\n",
    "# Diccionario auxiliar que contenga los colores de cada linea\n",
    "colors = {\n",
    "    \"A\": \"deepskyblue\",\n",
    "    \"B\": \"red\",\n",
    "    \"C\": \"blue\",\n",
    "    \"D\": \"green\",\n",
    "    \"E\": \"darkviolet\",\n",
    "    \"H\": \"yellow\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orden_linea[\"A\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colors[\"A\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = ut.get_all_data(molientes_path_old = molientes_paths_olds, molinetes_path_new = molientes_paths_news)\n",
    "# Version simplificada sin los datos viejos\n",
    "df = ut.get_all_data(molientes_path_old=None, molinetes_path_new=molientes_paths_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las columnas PAX_PAGOS, PAX_PASES_PAGOS y PAX_FREQ no las utilizaremos, asi que vamos a eliminarlas para ahorrar memoria.\n",
    "# Trabajaremos directamente con la columna TOTAL que es la suma de las tres anteriores.\n",
    "df.drop(columns=[\"PAX_PAGOS\", \"PAX_PASES_PAGOS\", \"PAX_FREQ\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "=== NOTA: Como importar funciones auxiliares ===\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si observamos, al principio de la notebook hemos importado una libreria llamada `utils.py`, la cual contiene funciones auxiliares que nos permiten mantener el código/notebook más limpio y organizado.\n",
    "\n",
    "En `utils.py` se declararon las funciones `get_all_data`, `load_subte_data_new`, `load_subte_data_old` y `clean_subte_dataframes`. \n",
    "\n",
    "Como se eligio el alias `ut` para importar `utils.py`, ahora podemos acceder a cualquiera de las funciones definidas en `utils.py` utilizando el prefijo `ut.` seguido del nombre de la función.\n",
    "\n",
    "En particular, como `ut.get_all_data` ya incluye la carga y limpieza de los datos, podemos utilizarla directamente para obtener el DataFrame consolidado y limpio.\n",
    "\n",
    "Veamos a continuación una breve descripción de las funciones definidas en `utils.py`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_all_data(molientes_path_old, molinetes_path_new):\n",
    "    \"\"\"\n",
    "    Función que unifica la carga y limpieza de todos los datos del subte.\n",
    "    \"\"\"\n",
    "    # === PROCESAMIENTO CONDICIONAL DE DATOS ===\n",
    "    # Solo procesamos datos antiguos si están disponibles\n",
    "    if molientes_path_old:             \n",
    "        # Cargamos los datos antiguos usando la función específica\n",
    "        df_old = load_subte_data_old(molientes_path_old)\n",
    "        # Aplicamos el proceso de limpieza a los datos antiguos\n",
    "        df_old = clean_subte_dataframes(df_old)\n",
    "        \n",
    "    # Solo procesamos datos nuevos si están disponibles\n",
    "    if molinetes_path_new:\n",
    "        # Cargamos los datos nuevos usando la función específica\n",
    "        df_new = load_subte_data_new(molinetes_path_new)\n",
    "        # Aplicamos el proceso de limpieza a los datos nuevos\n",
    "        df_new = clean_subte_dataframes(df_new)\n",
    "        \n",
    "    # === CONSOLIDACIÓN FINAL ===\n",
    "    # CODIGO OMITIDO (ver utils.py)\n",
    "\n",
    "    return df\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def load_subte_data_new(molinetes_path_new):\n",
    "    \"\"\"\n",
    "    Función para cargar y procesar datos nuevos de molinetes del subte.\n",
    "    Esta función maneja archivos más recientes que pueden estar organizados en carpetas.    \n",
    "    \"\"\"\n",
    "    # Diccionario para almacenar los DataFrames de cada año\n",
    "    dict_of_dataframes_news = {}\n",
    "    # Lista para almacenar información sobre las columnas (para debugging)\n",
    "    list_of_colums = []\n",
    "    # Columnas que queremos eliminar (incluye caracteres especiales por encoding)\n",
    "    delet_cols = [ 'Â¿Fuera de rango?', 'Ã¹Fuera de rango?', 'DIA','HORA','TIPO_DIA']\n",
    "    # Mapeo para renombrar columnas y estandarizar nombres\n",
    "    rename_cols = [\n",
    "    ('fecha', 'FECHA'), \n",
    "    ]\n",
    "\n",
    "    # Iteramos sobre cada año y su carpeta correspondiente\n",
    "    for year, folder in molinetes_path_new.items():\n",
    "        # root: carpeta actual, dirs: subcarpetas, files: archivos en la carpeta actual\n",
    "        for root, dirs, files in os.walk(folder):\n",
    "            # Procesamos cada archivo en la carpeta actual\n",
    "            for file in files:\n",
    "                # Solo procesamos archivos CSV\n",
    "                if file.endswith(\".csv\"):\n",
    "                    current_df = pd.read_csv(os.path.join(root, file), delimiter=';', engine='python', encoding='latin-1')\n",
    "                    # CODIGO OMITIDO (ver utils.py)\n",
    "                # Concatenamos el DataFrame actual con el DataFrame acumulado del año\n",
    "                df = pd.concat([df, current_df], ignore_index=True)\n",
    "            # Guardamos el DataFrame consolidado del año\n",
    "            dict_of_dataframes_news[year] = df\n",
    "    # Concatenamos todos los años en un único DataFrame\n",
    "    df_concat = pd.concat(dict_of_dataframes_news.values(), ignore_index=True)\n",
    "\n",
    "    return df_concat\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def clean_subte_dataframes(df):\n",
    "    \"\"\"\n",
    "    Función para limpiar y estandarizar los datos del subte.\n",
    "    Realiza múltiples operaciones de limpieza en las columnas LINEA y ESTACION,\n",
    "    maneja valores nulos y agrega columnas de tiempo útiles para el análisis.\n",
    "    \"\"\"\n",
    "    \n",
    "    # === LIMPIEZA DE LA COLUMNA LINEA ===\n",
    "    df[\"LINEA\"] = df[\"LINEA\"].str.replace(r'Linea?', '', regex=True)\n",
    "    df[\"LINEA\"] = df[\"LINEA\"].str.replace(r'LINEA_?', '', regex=True)\n",
    "    \n",
    "    # === LIMPIEZA DE LA COLUMNA ESTACION ===\n",
    "    df[\"ESTACION\"] = df[\"ESTACION\"].str.upper()\n",
    "    \n",
    "    # Lista de diferentes formas en que aparece \"AGÜERO\" debido a problemas de encoding\n",
    "    # Estos caracteres raros aparecen cuando hay problemas con acentos y caracteres especiales\n",
    "    aguero_replace = ['AGÃ¼ERO', 'AGÃ\\x83Â¼ERO' ,'AGÂ\\x81ERO', 'AGÃ\\x83Â\\x83Ã\\x82Â¼ERO' ,  'AGÏ¿½ERO', 'AGÃ\\x82Â³ERO' ,  'AGÃ¯Â¿Â½ERO',  'AGÃ\\x82Â\\x81ERO']\n",
    "    \n",
    "    # Reemplazamos todas las variaciones problemáticas de \"AGÜERO\" con la forma estándar\n",
    "    for a in aguero_replace:\n",
    "        df[\"ESTACION\"] = df[\"ESTACION\"].str.replace(a, 'AGUERO')\n",
    "    \n",
    "    # Estandarizamos otros nombres de estaciones problemáticos\n",
    "    # .*SAENZ.* captura cualquier variación que contenga \"SAENZ\"\n",
    "    df[\"ESTACION\"] = df[\"ESTACION\"].replace(r'.*SAENZ .*', 'SAENZ PEÑA', regex=True)\n",
    "    \n",
    "    # Removemos sufijos específicos de algunas estaciones (probablemente códigos internos)\n",
    "    df[\"ESTACION\"] = df[\"ESTACION\"].replace('CALLAO.B', 'CALLAO')\n",
    "    # CODIGO OMITIDO (ver utils.py)\n",
    "    \n",
    "    # === ANÁLISIS DE VALORES NULOS ===\n",
    "    \n",
    "    # === ELIMINACIÓN DE FILAS CON VALORES NULOS ===\n",
    "\n",
    "    # === CREACIÓN DE COLUMNAS TEMPORALES ===\n",
    "    df['MONTH'] = df['FECHA'].dt.month      # Mes (1-12)\n",
    "    df['YEAR'] = df['FECHA'].dt.year        # Año\n",
    "    df['DAY'] = df['FECHA'].dt.weekday      # Día de la semana (0=Lunes, 6=Domingo)\n",
    "    \n",
    "    return df\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para mas detalles, ver y/o modificar `utils.py`.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linea_estacion_df = df[[\"LINEA\", \"ESTACION\"]].drop_duplicates()\n",
    "linea_estacion_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suma por ESTACION/LINEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupa el DataFrame por estación y línea, luego suma la columna 'TOTAL' para cada grupo\n",
    "# Esto nos permite ver cuántos pasajeros totales pasaron por cada estación\n",
    "sum_by_ESTACIONES = df.groupby([\"ESTACION\", \"LINEA\"])[\"TOTAL\"].sum().reset_index()\n",
    "\n",
    "# Crea una lista ordenada de estaciones según el total de pasajeros (de mayor a menor)\n",
    "# Esta lista se utilizará más adelante para ordenar las estaciones en las visualizaciones\n",
    "order_by_ESTACIONES = sum_by_ESTACIONES.sort_values(\"TOTAL\", ascending=False)[\"ESTACION\"].tolist()\n",
    "\n",
    "# Calcula el porcentaje que representa cada estación sobre el total de pasajeros\n",
    "# La función sum() sin parámetros suma todos los valores de la columna 'TOTAL'\n",
    "sum_by_ESTACIONES['%TOTAL'] = sum_by_ESTACIONES['TOTAL'] / sum_by_ESTACIONES['TOTAL'].sum() * 100\n",
    "\n",
    "# Redondea los porcentajes a 2 decimales para mejorar la legibilidad\n",
    "sum_by_ESTACIONES[\"%TOTAL\"] = sum_by_ESTACIONES[\"%TOTAL\"].round(2)\n",
    "\n",
    "# Save sum_by_ESTACIONES\n",
    "sum_by_ESTACIONES.to_csv(\"sum_by_ESTACIONES.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(21, 7))\n",
    "sns.barplot(\n",
    "    data=sum_by_ESTACIONES,\n",
    "    x=\"ESTACION\",\n",
    "    y=\"TOTAL\",\n",
    "    order=order_by_ESTACIONES,\n",
    "    hue=sum_by_ESTACIONES[\"LINEA\"],\n",
    "    dodge=False,\n",
    "    palette=colors,\n",
    ")\n",
    "plt.title(\"TOTAL PASAJEROS POR ESTACIONES (2017-2023)\")\n",
    "plt.xticks(rotation=90)\n",
    "# guardemos la imagen en un archivo\n",
    "plt.savefig(\"total_pasajeros_por_estacion.png\")\n",
    "plt.show()\n",
    "\n",
    "# same plot but in % of the total\n",
    "plt.figure(figsize=(21, 7))\n",
    "sns.barplot(\n",
    "    data=sum_by_ESTACIONES,\n",
    "    x=\"ESTACION\",\n",
    "    y=\"%TOTAL\",\n",
    "    order=order_by_ESTACIONES,\n",
    "    hue=sum_by_ESTACIONES[\"LINEA\"],\n",
    "    dodge=False,\n",
    "    palette=colors,\n",
    ")\n",
    "plt.title(\"TOTAL PASAJEROS POR ESTACIONES (2017-2023)\")\n",
    "plt.xticks(rotation=90)\n",
    "# guardemos la imagen en un archivo\n",
    "plt.savefig(\"porcentaje_pasajeros_por_estacion.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_by_LINEA = df.groupby(\"LINEA\")[\"TOTAL\"].sum().reset_index()\n",
    "sum_by_LINEA[\"%TOTAL\"] = sum_by_LINEA[\"TOTAL\"] / sum_by_LINEA[\"TOTAL\"].sum() * 100\n",
    "# Para guardar el DataFrame en un archivo CSV ...\n",
    "sum_by_LINEA.to_csv(\"sum_by_LINEA.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot bar chart\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x=\"LINEA\", y=\"TOTAL\", data=sum_by_LINEA, hue=sum_by_LINEA[\"LINEA\"], palette=colors\n",
    ")\n",
    "plt.title(\"TOTAL PASAJEROS POR LINEA (2017-2023)\")\n",
    "# save plot\n",
    "plt.savefig(\"total_pasajeros_por_linea.png\")\n",
    "plt.show()\n",
    "# plot by percentage\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(\n",
    "    x=\"LINEA\", y=\"%TOTAL\", data=sum_by_LINEA, hue=sum_by_LINEA[\"LINEA\"], palette=colors\n",
    ")\n",
    "plt.title(\"PORCENTAJE DE PASAJEROS POR LINEA (2017-2023)\")\n",
    "# save plot\n",
    "plt.savefig(\"porcentaje_pasajeros_por_linea.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Definicion\n",
    "\n",
    "$$g_1=\\frac{m_3}{m_2^{3/2}}$$\n",
    "\n",
    "$$m_i=\\frac{1}{N}\\sum_{n=1}^N(x[n]-\\bar{x})^i\\$$\n",
    "\n",
    "* Sin sesgo\n",
    "\n",
    "$$G_1=\\frac{k_3}{k_2^{3/2}}=\\frac{\\sqrt{N(N-1)}}{N-2}\\frac{m_3}{m_2^{3/2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![Skewness Diagrams](https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Negative_and_positive_skew_diagrams_%28English%29.svg/800px-Negative_and_positive_skew_diagrams_%28English%29.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convertir la columna 'DESDE' a minutos desde la medianoche\n",
    "df[\"DESDE_MINUTOS\"] = pd.to_timedelta(df[\"DESDE\"]).dt.total_seconds() / 60\n",
    "df = df[(df[\"DESDE_MINUTOS\"] >= 360) & (df[\"DESDE_MINUTOS\"] <= 1439)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para cada línea única, recorrer cada estación\n",
    "# y calcular skewness de DESDE_MINUTOS, guardarlos en una lista, para un dataframe final\n",
    "skew_por_estacion = []\n",
    "for linea in df[\"LINEA\"].unique():\n",
    "    print(f\"Calculando skewness para {linea}\")\n",
    "    for estacion in df[df[\"LINEA\"] == linea][\"ESTACION\"].unique():\n",
    "        # obtener solo ESTACION y DESDE_MINUTOS del dataframe\n",
    "        partial_df = df[(df[\"LINEA\"] == linea) & (df[\"ESTACION\"] == estacion)][\n",
    "            [\"ESTACION\", \"DESDE_MINUTOS\", \"DAY\", \"TOTAL\"]\n",
    "        ]\n",
    "        # eliminar días de fin de semana (el simbolo ~ es para negar la condición)\n",
    "        partial_df = partial_df[~partial_df[\"DAY\"].isin([5, 6])]\n",
    "        # repetir las filas de acuerdo al valor de 'TOTAL'\n",
    "        partial_df = partial_df.loc[partial_df.index.repeat(partial_df[\"TOTAL\"])]\n",
    "        # calcular skewness de DESDE_MINUTOS\n",
    "        skew_value = skew(partial_df[\"DESDE_MINUTOS\"])\n",
    "        skew_por_estacion.append([linea, estacion, skew_value])\n",
    "\n",
    "# creamos un dataframe a partir de la lista\n",
    "skew_df = pd.DataFrame(skew_por_estacion, columns=[\"LINEA\", \"ESTACION\", \"SKEW\"])\n",
    "# guardamos skew_df\n",
    "skew_df.to_csv(\"skew_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos el dataframe skew_df\n",
    "skew_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(21, 7))\n",
    "sns.barplot(\n",
    "    data=skew_df,\n",
    "    x=\"ESTACION\",\n",
    "    y=\"SKEW\",\n",
    "    hue=skew_df[\"LINEA\"],\n",
    "    dodge=False,\n",
    "    order=order_by_ESTACIONES,\n",
    "    palette=colors,\n",
    ")\n",
    "# para agregar lineas horizontales en y=0.5 y y=-0.5 para tener de referencia\n",
    "plt.axhline(y=0.5, color=\"k\", linestyle=\"--\")\n",
    "plt.axhline(y=-0.5, color=\"k\", linestyle=\"--\")\n",
    "plt.title(\"SKEWNESS PASAJEROS POR ESTACIONES (2017-2023)\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pregunta**: Que podemos decir de la estacion \n",
    "\n",
    "* `Constitucion`(C)?\n",
    "* `Peru` (A)?\n",
    "* `Once` (H)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cantidad de pasajeros por estacion/horario/linea\n",
    "\n",
    "Originalmente los datos viene en franjas de 15 minutos, pero para este análisis los vamos a agrupar en franjas de 45 minutos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop DESDE_MINUTOS column\n",
    "df.drop(\"DESDE_MINUTOS\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos `delta` como el intervalo de tiempo en minutos para agrupar los datos\n",
    "delta = 45\n",
    "\n",
    "# Creamos una columna datetime combinando la fecha y la hora de inicio\n",
    "# Esto nos permitirá manipular mejor los datos temporales\n",
    "df[\"DESDE_DT\"] = pd.to_datetime(df[\"FECHA\"].astype(str) + \" \" + df[\"DESDE\"])\n",
    "\n",
    "# Nos aseguramos que el intervalo no sea menor a 15 minutos\n",
    "# Esto es importante porque los datos originales ya vienen en franjas de 15 minutos\n",
    "delta = max(15, delta)\n",
    "\n",
    "# Redondeamos la hora hacia abajo al intervalo de delta minutos más cercano\n",
    "# Por ejemplo, 8:37 con delta=45 se redondeará a 8:30\n",
    "df[\"DESDE_DT\"] = df[\"DESDE_DT\"].dt.floor(f\"{delta}T\")\n",
    "\n",
    "# Extraemos solo la parte de la hora (sin la fecha) y actualizamos la columna\n",
    "# Esto nos permitirá agrupar por hora del día independientemente de la fecha\n",
    "df[\"DESDE_DT\"] = df[\"DESDE_DT\"].dt.time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for linea in df[\"LINEA\"].unique():\n",
    "    # Filtramos el DataFrame para obtener solo los datos de la línea actual\n",
    "    df_linea = df[df[\"LINEA\"] == linea]\n",
    "    # Verificamos que todas las estaciones únicas para esta línea estén en la lista predefinida orden_linea\n",
    "    # Esta verificación asegura que nuestros datos coincidan con el orden esperado de estaciones\n",
    "    assert set(df_linea[\"ESTACION\"].unique()) == set(orden_linea[linea])\n",
    "\n",
    "    # Eliminamos sábados y domingos (donde DAY es 5 o 6)\n",
    "    # Nos enfocamos solo en días laborables para analizar patrones habituales\n",
    "    df_linea = df_linea[~df_linea[\"DAY\"].isin([5, 6])]\n",
    "\n",
    "    # Agrupamos los datos por estación y horario, sumando el total de pasajeros\n",
    "    estaciones_hasta_df = df_linea.groupby(['ESTACION', 'DESDE_DT' ])['TOTAL'].sum().reset_index()\n",
    "\n",
    "    # Eliminamos datos entre las 00:00 y las 05:30 (horario nocturno con poco tráfico)\n",
    "    estaciones_hasta_df = estaciones_hasta_df[\n",
    "        ~estaciones_hasta_df[\"DESDE_DT\"].between(\n",
    "            pd.to_datetime(\"00:00:00\").time(), pd.to_datetime(\"05:30:00\").time()\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Reducimos la escala dividiendo por 1 millón para mejor visualización\n",
    "    estaciones_hasta_df[\"TOTAL\"] = estaciones_hasta_df[\"TOTAL\"] / 1e6\n",
    "\n",
    "    # Creamos una figura de tamaño grande para mejor visualización\n",
    "    plt.figure(figsize=(30, 7))\n",
    "\n",
    "    # Generamos un gráfico de barras con seaborn\n",
    "    # Usamos el orden predefinido de estaciones para mantener coherencia geográfica\n",
    "    sns.barplot(\n",
    "        data=estaciones_hasta_df,\n",
    "        x=\"ESTACION\",\n",
    "        y=\"TOTAL\",\n",
    "        order=orden_linea[linea],\n",
    "        hue=estaciones_hasta_df[\"DESDE_DT\"],\n",
    "        palette=\"magma_r\",\n",
    "    )\n",
    "\n",
    "    # Agregamos título al gráfico\n",
    "    plt.title(f\"TOTAL PASAJEROS POR ESTACIONES DE LA LINEA {linea} Y SEGÚN HORARIO (2017-2023)\", size=18)\n",
    "\n",
    "    # Rotamos las etiquetas del eje X para mejor lectura\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Establecemos el límite del eje Y entre 0 y 4.5 millones\n",
    "    plt.ylim(0, 4.5)\n",
    "\n",
    "    # Etiquetamos los ejes\n",
    "    plt.ylabel(\"TOTAL PASAJEROS (Millones)\", size=18)\n",
    "    plt.xlabel(\"ESTACIONES\", size=18)\n",
    "\n",
    "    # Colocamos la leyenda en la parte superior con 18 columnas para mostrar todos los horarios\n",
    "    plt.legend(loc=\"upper center\", ncol=18)\n",
    "\n",
    "    # Guardamos el gráfico como archivo .png\n",
    "    plt.savefig(f\"horario_linea_{linea}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    # Mostramos el gráfico\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discusión sobre los resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veamos lo mismo, pero en terminos de porcentajs relativos a cada linea..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for linea in df[\"LINEA\"].unique():\n",
    "    # Filtramos el DataFrame para obtener solo los datos de la línea actual\n",
    "    df_linea = df[df[\"LINEA\"] == linea]\n",
    "\n",
    "    # Eliminamos sábados y domingos (donde DAY es 5 o 6)\n",
    "    df_linea = df_linea[~df_linea[\"DAY\"].isin([5, 6])]\n",
    "\n",
    "    # Agrupamos los datos por estación y horario, sumando el total de pasajeros\n",
    "    estaciones_hasta_df = df_linea.groupby([\"ESTACION\", \"DESDE_DT\"])[\"TOTAL\"].sum().reset_index()\n",
    "\n",
    "    # Eliminamos datos entre las 00:00 y las 05:30 (horario nocturno con poco tráfico)\n",
    "    estaciones_hasta_df = estaciones_hasta_df[\n",
    "        ~estaciones_hasta_df[\"DESDE_DT\"].between(\n",
    "            pd.to_datetime(\"00:00:00\").time(), pd.to_datetime(\"05:30:00\").time()\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Calculamos el porcentaje de cada combinación estación-horario sobre el total de la línea\n",
    "    estaciones_hasta_df[\"% TOTAL\"] = estaciones_hasta_df[\"TOTAL\"] / estaciones_hasta_df[\"TOTAL\"].sum() * 100\n",
    "\n",
    "    # Creamos una figura de tamaño grande para mejor visualización\n",
    "    plt.figure(figsize=(30, 7))\n",
    "\n",
    "    # Generamos un gráfico de barras con seaborn que muestra el porcentaje de pasajeros por estación\n",
    "    # Usamos el orden predefinido de estaciones para mantener coherencia geográfica\n",
    "    sns.barplot(\n",
    "        data=estaciones_hasta_df,\n",
    "        x=\"ESTACION\",\n",
    "        y=\"% TOTAL\",\n",
    "        order=orden_linea[linea],\n",
    "        hue=estaciones_hasta_df[\"DESDE_DT\"],\n",
    "        palette=\"magma_r\",\n",
    "    )\n",
    "\n",
    "    # Agregamos título al gráfico\n",
    "    plt.title(f\"% TOTAL DE PASAJEROS POR ESTACIONES DE LA LÍNEA {linea} Y SEGÚN HORARIO (2017-2023)\", size=18)\n",
    "\n",
    "    # Rotamos las etiquetas del eje X para mejor lectura\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Etiquetamos los ejes\n",
    "    plt.ylabel(\"% TOTAL PASAJEROS\", size=18)\n",
    "    plt.xlabel(\"ESTACIONES\", size=18)\n",
    "\n",
    "    # Colocamos la leyenda en la parte superior con 18 columnas para mostrar todos los horarios\n",
    "    plt.legend(loc=\"upper center\", ncol=18)\n",
    "    # Guardamos el gráfico como archivo .png\n",
    "    plt.savefig(f\"horario_linea_{linea}_percentage.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discusión sobre los resultados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caba-2027-eda-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
