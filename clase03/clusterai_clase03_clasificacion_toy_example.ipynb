{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "471bd253",
   "metadata": {},
   "source": [
    "____\n",
    "__Universidad Tecnológica Nacional, Buenos Aires__<br/>\n",
    "__Ingeniería Industrial__<br/>\n",
    "__Cátedra de Ciencia de Datos - Curso I5521__<br/>\n",
    "__Aprendizaje supervisado: Clasificacion__<br/>\n",
    "__Elaborado por: Martin Palazzo__ <br/>\n",
    "__Editado por: Nicolas Aguirre__ <br/>\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318a7e0d-3d9f-4416-a0ff-ae5dad318e96",
   "metadata": {},
   "source": [
    "### **Contexto del problema**\n",
    "\n",
    "Supongamos que tenemos un dataset de 2 features y 2 clases. Una clase es nuestras muestras de control y otra clase es la muestra de tratamiento. Supongan que quisieramos encontrar un modelo clasificador que logre discriminar entre ambas clases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0ce83c-fd8f-4b1c-98c8-d30e83926238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos las librerías necesarias para trabajar.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# importamos librerias de scikit learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import cross_validate, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dab737c-7f12-4eeb-95c5-2b4c8e137544",
   "metadata": {},
   "source": [
    "#### **1) creamos un dataset sintetico con Scikit Learn compuesto de 2 clases**\n",
    "El dataset esta compuesto de de 2 variables y 100 muestras con la funcion de sklearn \"make classification\". De todos modos pueden modificar estos parametros para generar distintos datasets. Por esta razon es que no es necesario importar un .csv, ya que el dataset esta generado sinteticamente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55a30ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cantidad de muestras \n",
    "nsamples = 200\n",
    "\n",
    "# cantidad de features\n",
    "nfeatures = 2\n",
    "\n",
    "x, y = make_classification(n_samples = nsamples, n_features=nfeatures, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1, class_sep = 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fd1759",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0682d39c-2bab-4335-8fbd-6fbd8683b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420dbfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5683deb-2378-45f6-9414-ac385237d622",
   "metadata": {},
   "source": [
    "### **Visualizamos el dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd0338c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizamos la matriz de variables\n",
    "sns.heatmap(x)\n",
    "plt.ylabel('samples')\n",
    "plt.xlabel('features')\n",
    "plt.title('Data Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616d45f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observamos el vector de labels\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e9d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# al ser un dataset de 2 dimensiones podemos visualizarlo con un scatterplot\n",
    "sns.set_context('talk')\n",
    "sns.scatterplot(x = x[:,0], y = x[:,1], hue = y)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.title('Visualizacion Datos')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da1092-e491-4ea6-8cc3-bc969da3630e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculamos la matriz de correlacion \n",
    "corr_matrix = np.corrcoef(x.T)\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ce5bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"talk\")\n",
    "sns.set_style(\"ticks\")\n",
    "sns.heatmap(corr_matrix)\n",
    "plt.title('Matriz de Correlacion Lineal de Pearson')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a963473-a200-4f7c-a877-d399da664c9d",
   "metadata": {},
   "source": [
    "### **Preparamos los datos para aprendizaje supervisado: clasificacion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a824e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos xtrain, xtest, ytrain e ytest :) \n",
    "# en este caso las muestras de test son el 20% del dataset. \n",
    "xtr, xte, ytr, yte = train_test_split(x, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5336b7d1-483b-4461-aef4-f84e79f106c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verificamos las dimensiones de la matriz de train\n",
    "xtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1576ff81-5428-4cb3-a54a-d5ac1f9afdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "xte.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255a0c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaramos un standard scaler y lo ajustamos a los datos de entrenamiento\n",
    "scaler = preprocessing.StandardScaler().fit(xtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f50524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformamos nuestros datos de entrenamiento y de test con la regla de scaling que aprendimos en el paso anterior\n",
    "xtr_scal = scaler.transform(xtr)  \n",
    "xte_scal = scaler.transform(xte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f9ad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizamos como queda el dataset pre y post scaling tanto en train como en test\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16,5))\n",
    "sns.heatmap(x, ax = axs[0])\n",
    "sns.heatmap(xtr_scal, ax = axs[1])\n",
    "sns.heatmap(xte_scal, ax = axs[2])\n",
    "axs[0].set_title('Raw data')\n",
    "axs[1].set_title('Standarized train data')\n",
    "axs[2].set_title('Standarized test data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88850bde-addb-4485-a367-3e25940f9e80",
   "metadata": {},
   "source": [
    "### **Entrenamiento del modelo de clasificacion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ad8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defino un modelo de clasificacion, en este caso un Logistic Regression importado de scikit learn\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# realizo un cross validation de 5 folds para ver la performance del modelo con distintas particiones de entrenamiento\n",
    "cv_results = cross_validate(lr, xtr_scal, ytr, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442d31dd-0cdd-419b-8c5a-e8462b85e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analizamos los resultados de cada iteracion del cross validation. \n",
    "# Este objeto indica el tiempo que tomo cada iteracion, y en \"test_score\" cuanto dio el resultado de accuracy\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94cb077-5f06-4d30-bcfd-4e84c98bfb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estudiamos como fue el promedio de todos los accuracys a lo largo de todas las iteraciones de cross validation\n",
    "np.mean(cv_results['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f7f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora entreno mi modelo utilizando todas las muestras de training utilizando tanto Xtrain como Ytrain\n",
    "lr.fit(xtr_scal, ytr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8096d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# observamos los coeficientes W del modelo luego del entrenamiento, un coeficiente por cada feature\n",
    "lr.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c14d346",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d34c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entonces nuestra sigmoid queda definida como\n",
    "def custom_sigmoid(z):\n",
    "    # W.x\n",
    "    x = lr.intercept_[0] + lr.coef_[0][0]*z[0] + lr.coef_[0][1]*z[1]\n",
    "    # sigmoid\n",
    "    y = 1 / (1 + np.exp(-x))\n",
    "    return y, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9a1a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si aplicamos entonces los parametros fitteados ...\n",
    "y_x_ = np.array([custom_sigmoid(z) for z in xte_scal])\n",
    "y_x_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68cea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero predicciones con mi modelo entrenado para las muestras de Test, utilizando solo Xtest\n",
    "ypred = lr.predict(xte_scal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2424e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computo el accuracy (comparar ytest vs ypred)\n",
    "test_acc = accuracy_score(yte, ypred)\n",
    "print(\"El accuracy es \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b37b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute and plot Area Under The Curve (AUC)\n",
    "yproba = lr.predict_proba(xte_scal)\n",
    "fpr1, tpr1, thresholds = roc_curve(yte.astype('int'), yproba[:,1], drop_intermediate = False)\n",
    "auc_ = auc(fpr1, tpr1)\n",
    "print(\"El AUC es = \" + str(auc_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74057307",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fpr1, tpr1, lw=2, alpha=0.7 , label = 'ROC curve', color = 'b')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', lw=1, color='r',label='Luck', alpha=.8)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.grid(False)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title('ROC curve with LR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71437a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos que es lo que esta sucendiendo con la sigmoid y los thresholds para armar la AUC ROC\n",
    "\n",
    "# Definamos algunos thresholds...\n",
    "thresholds = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "# Sigmoid para tener de referencia\n",
    "sigmoid_x = np.linspace(-10, 10, 100)\n",
    "sigmoid_y = 1 / (1 + np.exp(-sigmoid_x))\n",
    "\n",
    "\n",
    "# para cada uno de los thresholds ...\n",
    "for t in thresholds:\n",
    "    ypred_thresh = (yproba[:,1] >= t).astype('int')\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,6))\n",
    "    # ax1 Sigmoid curve with true labels\n",
    "    ax1.plot(sigmoid_x, sigmoid_y, label = 'Sigmoid', color = 'k')\n",
    "    sns.scatterplot(x=y_x_[:,1], y=yproba[:,1], hue=yte, edgecolor='k', ax=ax1)\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('Sigmoid(x)')\n",
    "    ax1.axhline(y=t, color='r', linestyle='--', lw=1, label = 'Threshold = ' + str(t))\n",
    "    ax1.axvline(x=0, color='k', linestyle='--', lw=1)\n",
    "    ax1.set_title('Sigmoid curve with thresholds')\n",
    "    ax1.legend()\n",
    "    # ax2 Thresholded predictions\n",
    "    ax2.plot(sigmoid_x, sigmoid_y, label = 'Sigmoid', color = 'k')\n",
    "    sns.scatterplot(x=y_x_[:,1], y=yproba[:,1], hue=ypred_thresh, edgecolor='k', ax=ax2)\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.axhline(y=t, color='r', linestyle='--', lw=1, label = 'Threshold = ' + str(t))\n",
    "    ax2.axvline(x=0, color='k', linestyle='--', lw=1)\n",
    "    ax2.set_title('Predictions with threshold = ' + str(t))\n",
    "    ax2.legend()\n",
    "    plt.show()\n",
    "\n",
    "    ####################\n",
    "    # Ejercicio:\n",
    "    ####################\n",
    "\n",
    "    # Para cada threshold calcular la confusion matrix y de ahi obtener:\n",
    "        # A) TP, TN, FP, FN\n",
    "        # B) Calcular TPR (Sensitivity) y FPR (1-Specificity)\n",
    "        # C) Graficar manualmente la curva ROC utilizando cada par de (TPR_t, FPR_t) para cada threshold t\n",
    "        # D) Validar la curva ROC obtenida con la funcion de sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817adb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion Matrix\n",
    "cm = confusion_matrix(yte, ypred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a7c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(cm, index = ['Tratamiento', 'Control'], columns = ['Tratamiento', 'Control'])\n",
    "plt.figure(figsize = (6,4))\n",
    "sns.heatmap(df_cm, annot=True)\n",
    "plt.title('Classification Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd866e5c-dfc8-4f77-811a-79839e4884ec",
   "metadata": {},
   "source": [
    "### **Tarea y proximos pasos**\n",
    "Repetir el ejecicio aunque esta vez con mas cantidad de features y distintas cantidades de muestras. Analizar como se comporta el clasificador y su performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c325d8ed-eb54-4650-85ea-69edbfad4ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### codigo aqui ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80264002-7934-44cc-9f3b-6691365399c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### codigo aqui ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aaab92-d513-4234-9470-d64575ab798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### codigo aqui ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e552c5-7568-4cff-bfdb-e75cce3e2144",
   "metadata": {},
   "outputs": [],
   "source": [
    "### codigo aqui ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba09a17-68b2-4133-a096-2d809cf49d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### codigo aqui ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "caba-2027-eda-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
